_type_: stp_experiment.se_offline
project: stp.se_offline_debug
log: True    # wandb logging, use 'dry' for dummy do-nothing logging to test metrics collection

seed: 42
setup: setups.binary_basic_sp
debug: False

train:
  n_epochs: 30
  batch_size: 64
test:
  eval_first: 2
  eval_schedule: 5
  n_epochs: 15
  noise: 0.0

classifier:
  classification: ???
  layers: ???
  n_extra_layers: []
  #  No norm
#  learning_rate: 0.0006
  #  L2
#  learning_rate: 0.02
  #  L1
  learning_rate: 0.06
  collect_losses: 1
  symexp_logits: ???

enable_autoencoding_task: False

sp:
  htm_sp:
    _type_: sp.htm
    seed: ???
    feedforward_sds: ???
    output_sds: ???
#    potential_synapses_ratio: 0.05
    potential_synapses_ratio: 0.1
#    potential_synapses_ratio: 0.25
#    potential_synapses_ratio: 1.0
    synapse_permanence_deltas: [0.14, 0.02]
    connected_permanence_threshold: 0.5
    boost_strength: 7.0
    boost_sliding_window: 1_500
    expected_normal_overlap_frequency: 0.2
    min_activation_threshold: 6

  soft_hebb:
    _type_: sp.soft_hebb
    seed: ???
    feedforward_sds: ???
    output_sds: ???
    weights_distribution: normal
    inhibitory_ratio: 0.5
    init_radius: 20.0
    learning_rate: 0.025
    adaptive_lr: True
    normalize_dw: True
    beta: 200.0
    beta_lr: 0.0
    threshold: 0.001

  soft_hebb_ext:
    _base_: sp.soft_hebb
    init_radius: 20.0
    learning_rate: 0.025
    adaptive_lr: True
    normalize_dw: False
    beta: 200.0
    beta_lr: 0.002
    threshold: 0.001
    output_extra: 1.0
    min_active_mass: 0.75
    min_mass: 0.92
    bias_boosting: True
    negative_hebbian: 'no'
    filter_output: 'hard'
    normalize_output: 'no'

  krotov:
    _type_: sp.krotov
    seed: ???
    feedforward_sds: ???
    output_sds: ???
    init_radius: 10.0
    learning_rate: 0.01
    lebesgue_p: 3
    neg_hebb_delta: 0.4
    repu_n: 4.5

  krotov_adaptive:
    _base_: sp.krotov
    adaptive_lr: True

  se:
    _type_: sp.se
    seed: ???
    feedforward_sds: ???
    output_sds: ???
    adapt_to_ff_sparsity: True

    normalize_input_p: 0.0
    # no | subtract_avg | subtract_avg_and_clip
    filter_input_policy: no

    # weights norm [should be compatible with learning policy, i.e.
    #   p = 1 for linear and p > 1 for krotov]
    lebesgue_p: 1.0
#    lebesgue_p: 2.0
    init_radius: 20.0
    weights_distribution: normal

    initial_rf_to_input_ratio: 100.0
    initial_max_rf_sparsity: 1.0

    # newborn phase
    pruning:
#      n_stages: 10
      n_stages: 0
      cycle: 200.0
      # linear | powerlaw
      mode: powerlaw
      target_max_rf_sparsity: 0.5
      target_rf_to_input_ratio: 0.75

#    synaptogenesis:
#      cycle: 100.0

    # Weights power for matching [should be compatible with learning policy, i.e.
    #   lebesgue_p - 1 for krotov (use ... to make it auto-induced), and
    #   we allow any p for linear policy (however, only p = 1 is the most natural)]
    # ... | <power>
    match_p: ...
    # For match_p != 1.0 it's always mul ==> u = Ax
    #   match_p == 1.0: mul or min, which is u = \sum_i pairwise_min(a_i, x)
    match_op: mul
#    match_op: min
    # no | additive | multiplicative
    boosting_policy: multiplicative
    min_boosting_k: 0.05
    # powerlaw | exponential
    activation_policy: powerlaw
    beta: 1.0
    beta_lr: 0.0
#    beta_lr: 0.0004
    # abs or relative to K
    soft_extra: 1.0
    # with respect to top K against second top K
    beta_active_mass: [0.72, 0.82]

    normalize_output: False
    output_extra: 10.0
#    output_extra: 4.0

    # linear | krotov
    learning_policy: linear
#    learning_policy: krotov
    learning_rate: 0.01
    adaptive_lr: True
    lr_range: [0.002, 0.1]
    # M | [M, Q]: M - hebb, Q - anti-hebb; ints or floats â€” absolute or relative to K
    learning_set: [0.75, 0.75]
    anti_hebb_scale: 0.8
    persistent_signs: False
    normalize_dw: False
    print_stats_schedule: 25_000

setups:
  rate_ann:
    input_mode: rate
    ds_norm: l2
    grayscale: True
#    grayscale: False
    contrastive: False
#    classifier_symexp_logits: True
    encoding_sds: [128, 1]
#    encoding_sds: [256, 1]
  rate_ann_2k:
    input_mode: rate
    ds_norm: l2
    encoding_sds: [2000, 1]
  rate_soft_hebb:
    encoder: sp.soft_hebb
    input_mode: rate
    ds_norm: l2
#    encoding_sds: [2000, 20]
    encoding_sds: [800, 8]
  rate_soft_hebb_ext:
    _base_: setups.rate_soft_hebb
    encoder: sp.soft_hebb_ext
    encoding_sds: [2000, 10]
#    encoding_sds: [800, 4]
  rate_krotov:
    encoder: sp.krotov
#    sdr_tracker: False
    input_mode: rate
    ds_norm: lp
    encoding_sds: [2000, 7]
#    encoding_sds: [800, 3]
  rate_krotov_adaptive:
    _base_: setups.rate_krotov
    encoder: sp.krotov_adaptive
  rate_se:
    encoder:
      _base_: sp.se
      output_mode: rate
    input_mode: rate
    ds_norm: lp
    grayscale: True
#    grayscale: False
    contrastive: False
#    contrastive: True
#    encoding_sds: [4000, 40]
#    encoding_sds: [2000, 20]
#    encoding_sds: [2000, 12]
#    encoding_sds: [1000, 10]
#    encoding_sds: [1000, 6]
#    encoding_sds: [600, 7]
    encoding_sds: [600, 4]
#    encoding_sds: [100, 2]
  binary_htm_sparse:
    encoder:
      _base_: sp.htm_sp
      potential_synapses_ratio: 0.05
    input_mode: binary
    encoding_sds: [2000, 40]
  binary_htm_dense:
    encoder:
      _base_: sp.htm_sp
      potential_synapses_ratio: 0.9
    input_mode: binary
    encoding_sds: [2000, 40]

data: mnist

sdr_tracker:
  _type_: tracker.sdr
  sds: ???

wandb_init:
  _base_: $_base.wandb_init

